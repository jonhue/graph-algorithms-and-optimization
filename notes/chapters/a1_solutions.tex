% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Solutions}

\section{\Cref{part1}}

\subsection{Electrical Flows}

\begin{proof}[Proof of \cref{clm:electrical_flow_optimization_convex}] We have that $\mH_c(\vx) = \mL(\vx) \succeq \mZero$.
\end{proof}

\begin{proof}[Solution to \cref{exc:electrical_flow_energy_minimizing}] Consider any $\vf \in \R^{|\sE|}$ satisfying $\mB\vf = \vd$. We have for any $\vx \in \R^{|\sV|}$, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf &= \frac{1}{2}\trans{\vf}\mR\vf - \trans{\vx}\underbrace{(\mB\vf - \vd)}_{0} \\
    &\geq \min_{\vf' \in \R^{|\sE|}} \underbrace{\frac{1}{2}\trans{\vf'}\mR\vf' - \trans{\vx}(\mB\vf' - \vd)}_{\eqdef g(\vf')}.
\end{align*} Note that $g$ is convex. Taking the gradient gives, \begin{align*}
    \grad_{\vf'} g(\vf') = \mR\vf' - \trans{\mB}\vx.
\end{align*} So $\vf' = \inv{\mR}\trans{\mB}\vx$ is the minimizer of $g$. We obtain, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf \geq -\frac{1}{2}\trans{\vx}\mL\vx + \trans{\vd}\vx,
\end{align*} but $\trans{\Tilde{\vf}}\mR\Tilde{\vf} = \trans{\Tilde{\vx}}\mL\Tilde{\vx} = \trans{\vd}\Tilde{\vx}$ for electrical voltages $\Tilde{\vx}$ and electrical flow $\Tilde{\vf}$ (see \cref{eq:electrical_energy_demands}). Thus, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf \geq \frac{1}{2}\trans{\Tilde{\vx}}\mL\Tilde{\vx} = \frac{1}{2}\trans{\Tilde{\vf}}\mR\Tilde{\vf}.
\end{align*} Hence, $\Tilde{\vf}$ is the minimum electrical energy flow among all flows routing $\vd$.
\end{proof}

\begin{proof}[Solution to \cref{exc:electrical_energy_strong_duality}] TBD
\end{proof}

\begin{proof}[Solution to \cref{exc:electrical_energy_weak_duality}] TBD
\end{proof}

\subsection{Linear Algebra}

\begin{proof}[Proof of \cref{clm:eigenvalues_eigenvectors_of_matrix}] Because the characteristic polynomial of $\mA$ is of degree $n$, it has $n$ complex roots, which are the eigenvalues $\lambda_1, \dots, \lambda_n$ of $\mA$. We will first prove that the $\lambda_i$ are real. Then, we will prove that the corresponding eigenvectors $\vv_i$ are orthogonal.

\begin{enumerate}
    \item Let $\lambda$ be any eigenvalue of $\mA$. We denote by $\Bar{\lambda}$ the complex conjugate of $\lambda$. Clearly, if $\lambda = \Bar{\lambda}$, then $\lambda \in \R$. By the definition of the eigenvalue $\lambda$ with associated eigenvector $\vv$, we have, \begin{align*}
        \lambda\trans{\Bar{\vv}}\vv = \trans{\Bar{\vv}}\mA\vv.
    \end{align*} Taking the complex conjugate and transpose of both sides gives, \begin{align*}
        \Bar{\lambda}\trans{\Bar{\vv}}\vv = \trans{\Bar{\vv}}\trans{\Bar{\mA}}\vv = \trans{\Bar{\vv}}\mA\vv = \lambda\trans{\Bar{\vv}}\vv. \margintag{using that $\mA$ is real and symmetric, $\trans{\Bar{\mA}} = \mA$}
    \end{align*} We have $\lambda = \Bar{\lambda}$ as desired.
    
    \item It remains to show that for eigenvalues $\lambda_i, \lambda_j$ with associated eigenvectors $\vv_i, \vv_j$ and $i \neq j$, we have $\trans{\vv_i}\vv_j = 0$. By the definition of an eigenvalue, we have, \begin{align*}
        \lambda_i\trans{\vv_j}\vv_i = \trans{\vv_j}\mA\vv_i &= \trans{(\trans{\vv_i}\trans{\mA}\vv_j)} \\ &= \trans{(\trans{\vv_i}\mA\vv_j)} = \lambda_j \trans{(\trans{\vv_i}\vv_j)} = \lambda_j\trans{\vv_j}\vv_i. \margintag{using that $\mA$ is symmetric}
    \end{align*} We get that $\trans{\vv_j}\vv_i = 0$ if $\lambda_i \neq \lambda_j$. \qedhere
\end{enumerate}
\end{proof}

\begin{proof}[Proof of \cref{clm:eigenvalues_of_matrix_product}]
Suppose $\lambda$ is an eigenvalue of $\mM$ with associated eigenvector $\vv$. Also suppose that $\mT\vv = \vw$. Then, \begin{align*}
    \mT\mM\inv{\mT}\vw = \mT\mM\vv = \lambda\mT\vv = \vw.
\end{align*} It is easy to check that the other direction holds too.
\end{proof}

\begin{proof}[Proof of \cref{clm:matrix_function_facts}] TBD
\end{proof}

\begin{proof}[Proof of \cref{clm:orthogonal_projection}] TBD
\end{proof}

\begin{proof}[Proof of \cref{clm:pinv_calculation}] TBD
\end{proof}

\subsection{Probability}

\begin{thm}[Jensen's inequality, finite form]\label{thm:jensens_inequality_finite_form}
Let $f: \sS \to \R$ be a convex function on the convex set $\sS \subseteq \R^n$. Suppose that $\vx_1, \dots, \vx_k \sS$ and $\theta_1, \dots, \theta_k \geq 0$ with $\theta_1 + \dots + \theta_k = 1$. Then, \begin{align}
    f(\theta_1\vx_1 + \dots + \theta_k\vx_k) \leq \theta_1 f(\vx_1) + \dots + \theta_k f(\vx_k).
\end{align}
\end{thm}
\begin{proof}
We prove the statement by induction on $k$. The base case, $k = 2$, follows trivially from the convexity of $f$. For the induction step, suppose that the statement holds for some $k \geq 2$. Assume w.l.o.g. that $\theta_{k+1} \in (0,1)$. We have, \begin{align*}
    \sum_{i=1}^{k+1} \theta_i f(\vx_i) &= (1-\theta_{k+1})\parentheses*{\sum_{i=1}^k \frac{\theta_i}{1 - \theta_{k+1}} f(\vx_i)} + \theta_{k+1} f(\vx_{k+1}) \\
    &\geq (1-\theta_{k+1})f\parentheses*{\sum_{i=1}^k \frac{\theta_i}{1-\theta_{k+1}}\vx_i} + \theta_{k+1}f(\vx_{k+1}) \margintag{using the induction hypothesis} \\
    &\geq f\parentheses*{\sum_{i=1}^{k+1} \theta_i \vx_i}. \margintag{using convexity of $f$} &\qedhere
\end{align*}
\end{proof}

\section{\Cref{part2}}

\subsection{Convex Geometry}

\begin{proof}[Proof of \cref{thm:extreme_value_theorem}] In our proof, we will use the following two theorems.
\begin{fct}[Bolzano-Weierstrass theorem]\index{Bolzano-Weierstrass theorem} Every bounded sequence in $\R^n$ has a convergent subsequence.
\end{fct}
\begin{fct}[Boundedness theorem]\index{Boundedness theorem}
Let $f : \R^n \to \R$ be a continuous function and $\spa{F} \subseteq \R^n$ be non-empty, bounded, and closed. Then $f$ is bounded on $\spa{F}$.
\end{fct}

Let $\alpha$ be the infimum of $f$ over $\spa{F}$, i.e., the largest value for which any $\vx \in \spa{F}$ satisfies $f(\vx) \geq \alpha$. By the boundedness theorem, the infumum exists, as $f$ is lower bounded and the set of lower bounds has a greatest lower bound, $\alpha$.

Let $\spa{F}_k \defeq \{\vx \in \spa{F} \mid \alpha \leq f(\vx) \leq \alpha + 2^{-k}\}$. $\spa{F}_k$ cannot be empty, since if it were, then $\alpha + 2^{-k}$ would be a strictly greater lower bound on $f$ than $\alpha$. For each $k$, let $\vx_k$ be some $\vx \in \spa{F}_k$. $\{\vx_k\}_{k=1}^\infty$ is a bounded sequence as $\spa{F}_k \subseteq \spa{F}$, so by the Bolzano-Weierstrass theorem, there exists a convergent subsequence, $\{\vy_k\}_{k=1}^\infty$, with limit $\Bar{\vy}$. Because the set is closed, $\Bar{\vy} \in \spa{F}$. By continuity, $f(\Bar{\vy}) = \lim_{k\to\infty} f(\vy_k)$, and by construction, $\lim_{k\to\infty} f(\vy_k) = \alpha$.

Thus, the optimal solution is $\Bar{\vy}$.
\end{proof}

\begin{proof}[Solution to \cref{exc:convex_iff_epi_convex}] TBD
\end{proof}

\begin{proof}[Solution to \cref{exc:sub_level_set_of_convex_function_convex}] TBD
\end{proof}

\section{\Cref{part3}}

\subsection{Introduction to Spectral Graph Theory}

\begin{proof}[Solution to \cref{exc:spectrum_path_graph_2}] TBD
\end{proof}

\begin{proof}[Solution to \cref{exc:spectrum_path_graph_n}] TBD
\end{proof}

\begin{proof}[Solution to \cref{exc:spectrum_complete_binary_tree_2}] TBD
\end{proof}

\begin{proof}[Solution to \cref{exc:spectrum_complete_binary_tree_n}] TBD
\end{proof}

\subsection{Conductance and Expanders}

\begin{proof}[Solution to \cref{exc:expander_complete_graph}] TBD
\end{proof}

\begin{proof}[Solution to \cref{exc:expander_path_graph}] TBD
\end{proof}