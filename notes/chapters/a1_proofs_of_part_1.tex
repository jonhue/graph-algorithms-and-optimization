% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Proofs of \Cref{part1}}

\section{Electrical Flows}

\begin{lem}\label{lem:a1}
$c(\vx) \defeq \frac{1}{2}\trans{\vx}\mL\vx - \trans{\vx}\vd$ is convex.
\end{lem}
\begin{proof} We have that $\mH_c(\vx) = \mL(\vx) \succeq \mZero$.
\end{proof}

\begin{lem}\label{lem:a2}
The electrical energy-minimizing flow $\s{\vf}$ from \cref{eq:electrical_energy_minimizing_flow} is the electrical flow, i.e., satisfies Ohm's law.
\end{lem}
\begin{proof} Consider any $\vf \in \R^{|\sE|}$ satisfying $\mB\vf = \vd$. We have for any $\vx \in \R^{|\sV|}$, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf &= \frac{1}{2}\trans{\vf}\mR\vf - \trans{\vx}\underbrace{(\mB\vf - \vd)}_{0} \\
    &\geq \min_{\vf' \in \R^{|\sE|}} \underbrace{\frac{1}{2}\trans{\vf'}\mR\vf' - \trans{\vx}(\mB\vf' - \vd)}_{\eqdef g(\vf')}.
\end{align*} Note that $g$ is convex. Taking the gradient gives, \begin{align*}
    \grad_{\vf'} g(\vf') = \mR\vf' - \trans{\mB}\vx.
\end{align*} So $\vf' = \inv{\mR}\trans{\mB}\vx$ is the minimizer of $g$. We obtain, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf \geq -\frac{1}{2}\trans{\vx}\mL\vx + \trans{\vd}\vx,
\end{align*} but $\trans{\Tilde{\vf}}\mR\Tilde{\vf} = \trans{\Tilde{\vx}}\mL\Tilde{\vx} = \trans{\vd}\Tilde{\vx}$ for electrical voltages $\Tilde{\vx}$ and electrical flow $\Tilde{\vf}$ (see \cref{eq:electrical_energy_demands}). Thus, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf \geq \frac{1}{2}\trans{\Tilde{\vx}}\mL\Tilde{\vx} = \frac{1}{2}\trans{\Tilde{\vf}}\mR\Tilde{\vf}.
\end{align*} Hence, $\Tilde{\vf}$ is the minimum electrical energy flow among all flows routing $\vd$.
\end{proof}

\begin{lem}\label{lem:a3}
For the electrical flow $\s{\vf}$ and electrical voltages $\s{\vx}$,\par\noindent $\mathcal{E}(\s{\vf}) = -c(\s{\vx})$.
\end{lem}
\begin{proof} TBD
\end{proof}

\begin{lem}\label{lem:a4}
For any flow $\vf$ routing $\vd$ and voltages $\vx$, $\mathcal{E}(\vf) \geq -c(\vx)$.
\end{lem}
\begin{proof} TBD
\end{proof}

\section{Linear Algebra}

\begin{thm}\label{thm:a5}
If a square matrix $\mA \in \R^{n \times n}$ is symmetric, then $\mA$ has $n$ real eigenvalues $\lambda_1, \dots, \lambda_n$ and eigenvectors $\vv_1, \dots, \vv_n \in \R^n$ such that $\mA\vv_i = \lambda_i\vv_i$ and the $\vv_i$ are orthogonal.
\end{thm}
\begin{proof} Because the characteristic polynomial of $\mA$ is of degree $n$, it has $n$ complex roots, which are the eigenvalues $\lambda_1, \dots, \lambda_n$ of $\mA$. We will first prove that the $\lambda_i$ are real. Then, we will prove that the corresponding eigenvectors $\vv_i$ are orthogonal.

\begin{enumerate}
    \item Let $\lambda$ be any eigenvalue of $\mA$. We denote by $\Bar{\lambda}$ the complex conjugate of $\lambda$. Clearly, if $\lambda = \Bar{\lambda}$, then $\lambda \in \R$. By the definition of the eigenvalue $\lambda$ with associated eigenvector $\vv$, we have, \begin{align*}
        \lambda\trans{\Bar{\vv}}\vv = \trans{\Bar{\vv}}\mA\vv.
    \end{align*} Taking the complex conjugate and transpose of both sides gives, \begin{align*}
        \Bar{\lambda}\trans{\Bar{\vv}}\vv = \trans{\Bar{\vv}}\trans{\Bar{\mA}}\vv = \trans{\Bar{\vv}}\mA\vv = \lambda\trans{\Bar{\vv}}\vv. \margintag{using that $\mA$ is real and symmetric, $\trans{\Bar{\mA}} = \mA$}
    \end{align*} We have $\lambda = \Bar{\lambda}$ as desired.
    
    \item It remains to show that for eigenvalues $\lambda_i, \lambda_j$ with associated eigenvectors $\vv_i, \vv_j$ and $i \neq j$, we have $\trans{\vv_i}\vv_j = 0$. By the definition of an eigenvalue, we have, \begin{align*}
        \lambda_i\trans{\vv_j}\vv_i = \trans{\vv_j}\mA\vv_i &= \trans{(\trans{\vv_i}\trans{\mA}\vv_j)} \\ &= \trans{(\trans{\vv_i}\mA\vv_j)} = \lambda_j \trans{(\trans{\vv_i}\vv_j)} = \lambda_j\trans{\vv_j}\vv_i. \margintag{using that $\mA$ is symmetric}
    \end{align*} We get that $\trans{\vv_j}\vv_i = 0$ if $\lambda_i \neq \lambda_j$. \qedhere
\end{enumerate}
\end{proof}

\begin{lem}\label{lem:a6}
Consider a real symmetric matrix $\mA = \mX\mY\trans{\mX}$, where $\mX$ is real and invertible and $\mY$ is real and symmetric. Let $\mPi_\mA$ denote the orthogonal projection to the image of $\mA$. Then, $\pinv{\mA} = \mPi_\mA\inv{(\trans{\mX})}\pinv{\mY}\inv{\mX}\mPi_\mA$.
\end{lem}
\begin{proof} TBD
\end{proof}

\begin{lem}\label{lem:a7} For any matrix $\mM$ and invertible matrix $\mT$, $\mM$ and $\mT\mM\inv{\mT}$ have the same eigenvalues.
\end{lem}
\begin{proof}
Suppose $\lambda$ is an eigenvalue of $\mM$ with associated eigenvector $\vv$. Also suppose that $\mT\vv = \vw$. Then, \begin{align*}
    \mT\mM\inv{\mT}\vw = \mT\mM\vv = \lambda\mT\vv = \vw.
\end{align*} It is easy to check that the other direction holds too.
\end{proof}

\section{Probability}

\begin{thm}[Jensen's inequality, finite form]\label{thm:a8}
Let $f: \sS \to \R$ be a convex function on the convex set $\sS \subseteq \R^n$. Suppose that $\vx_1, \dots, \vx_k \sS$ and $\theta_1, \dots, \theta_k \geq 0$ with $\theta_1 + \dots + \theta_k = 1$. Then, \begin{align}
    f(\theta_1\vx_1 + \dots + \theta_k\vx_k) \leq \theta_1 f(\vx_1) + \dots + \theta_k f(\vx_k).
\end{align}
\end{thm}
\begin{proof}
We prove the statement by induction on $k$. The base case, $k = 2$, follows trivially from the convexity of $f$. For the induction step, suppose that the statement holds for some $k \geq 2$. Assume w.l.o.g. that $\theta_{k+1} \in (0,1)$. We have, \begin{align*}
    \sum_{i=1}^{k+1} \theta_i f(\vx_i) &= (1-\theta_{k+1})\parentheses*{\sum_{i=1}^k \frac{\theta_i}{1 - \theta_{k+1}} f(\vx_i)} + \theta_{k+1} f(\vx_{k+1}) \\
    &\geq (1-\theta_{k+1})f\parentheses*{\sum_{i=1}^k \frac{\theta_i}{1-\theta_{k+1}}\vx_i} + \theta_{k+1}f(\vx_{k+1}) \margintag{using the induction hypothesis} \\
    &\geq f\parentheses*{\sum_{i=1}^{k+1} \theta_i \vx_i}. \margintag{using convexity of $f$} &\qedhere
\end{align*}
\end{proof}