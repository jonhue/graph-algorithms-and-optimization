% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Solutions of \Cref{part1}}

\section{Electrical Flows}

\begin{proof}[Proof of \cref{clm:electrical_flow_optimization_convex}] We have that $\mH_c(\vx) = \mL(\vx) \succeq \mZero$.
\end{proof}

\begin{proof}[Solution to \cref{exc:electrical_flow_energy_minimizing}] Consider any $\vf \in \R^{|\sE|}$ satisfying $\mB\vf = \vd$. We have for any $\vx \in \R^{|\sV|}$, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf &= \frac{1}{2}\trans{\vf}\mR\vf - \trans{\vx}\underbrace{(\mB\vf - \vd)}_{0} \\
    &\geq \min_{\vf' \in \R^{|\sE|}} \underbrace{\frac{1}{2}\trans{\vf'}\mR\vf' - \trans{\vx}(\mB\vf' - \vd)}_{\eqdef g(\vf')}.
\end{align*} Note that $g$ is convex. Taking the gradient gives, \begin{align*}
    \grad_{\vf'} g(\vf') = \mR\vf' - \trans{\mB}\vx.
\end{align*} So $\vf' = \inv{\mR}\trans{\mB}\vx$ is the minimizer of $g$. We obtain, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf \geq -\frac{1}{2}\trans{\vx}\mL\vx + \trans{\vd}\vx,
\end{align*} but $\trans{\Tilde{\vf}}\mR\Tilde{\vf} = \trans{\Tilde{\vx}}\mL\Tilde{\vx} = \trans{\vd}\Tilde{\vx}$ for electrical voltages $\Tilde{\vx}$ and electrical flow $\Tilde{\vf}$ (see \cref{eq:electrical_energy_demands}). Thus, \begin{align*}
    \frac{1}{2}\trans{\vf}\mR\vf \geq \frac{1}{2}\trans{\Tilde{\vx}}\mL\Tilde{\vx} = \frac{1}{2}\trans{\Tilde{\vf}}\mR\Tilde{\vf}.
\end{align*} Hence, $\Tilde{\vf}$ is the minimum electrical energy flow among all flows routing $\vd$.
\end{proof}

\begin{proof}[Solution to \cref{exc:electrical_energy_strong_duality}] TBD
\end{proof}

\begin{proof}[Solution to \cref{exc:electrical_energy_weak_duality}] TBD
\end{proof}

\section{Linear Algebra}

\begin{proof}[Proof of \cref{clm:eigenvalues_eigenvectors_of_matrix}] Because the characteristic polynomial of $\mA$ is of degree $n$, it has $n$ complex roots, which are the eigenvalues $\lambda_1, \dots, \lambda_n$ of $\mA$. We will first prove that the $\lambda_i$ are real. Then, we will prove that the corresponding eigenvectors $\vv_i$ are orthogonal.

\begin{enumerate}
    \item Let $\lambda$ be any eigenvalue of $\mA$. We denote by $\Bar{\lambda}$ the complex conjugate of $\lambda$. Clearly, if $\lambda = \Bar{\lambda}$, then $\lambda \in \R$. By the definition of the eigenvalue $\lambda$ with associated eigenvector $\vv$, we have, \begin{align*}
        \lambda\trans{\Bar{\vv}}\vv = \trans{\Bar{\vv}}\mA\vv.
    \end{align*} Taking the complex conjugate and transpose of both sides gives, \begin{align*}
        \Bar{\lambda}\trans{\Bar{\vv}}\vv = \trans{\Bar{\vv}}\trans{\Bar{\mA}}\vv = \trans{\Bar{\vv}}\mA\vv = \lambda\trans{\Bar{\vv}}\vv. \margintag{using that $\mA$ is real and symmetric, $\trans{\Bar{\mA}} = \mA$}
    \end{align*} We have $\lambda = \Bar{\lambda}$ as desired.
    
    \item It remains to show that for eigenvalues $\lambda_i, \lambda_j$ with associated eigenvectors $\vv_i, \vv_j$ and $i \neq j$, we have $\trans{\vv_i}\vv_j = 0$. By the definition of an eigenvalue, we have, \begin{align*}
        \lambda_i\trans{\vv_j}\vv_i = \trans{\vv_j}\mA\vv_i &= \trans{(\trans{\vv_i}\trans{\mA}\vv_j)} \\ &= \trans{(\trans{\vv_i}\mA\vv_j)} = \lambda_j \trans{(\trans{\vv_i}\vv_j)} = \lambda_j\trans{\vv_j}\vv_i. \margintag{using that $\mA$ is symmetric}
    \end{align*} We get that $\trans{\vv_j}\vv_i = 0$ if $\lambda_i \neq \lambda_j$. \qedhere
\end{enumerate}
\end{proof}

\begin{proof}[Proof of \cref{clm:eigenvalues_of_matrix_product}]
Suppose $\lambda$ is an eigenvalue of $\mM$ with associated eigenvector $\vv$. Also suppose that $\mT\vv = \vw$. Then, \begin{align*}
    \mT\mM\inv{\mT}\vw = \mT\mM\vv = \lambda\mT\vv = \vw.
\end{align*} It is easy to check that the other direction holds too.
\end{proof}

\begin{proof}[Proof of \cref{clm:matrix_function_facts}] TBD
\end{proof}

\begin{proof}[Proof of \cref{clm:orthogonal_projection}] TBD
\end{proof}

\begin{proof}[Proof of \cref{clm:pinv_calculation}] TBD
\end{proof}

\section{Probability}

\begin{thm}[Jensen's inequality, finite form]\label{thm:jensens_inequality_finite_form}
Let $f: \sS \to \R$ be a convex function on the convex set $\sS \subseteq \R^n$. Suppose that $\vx_1, \dots, \vx_k \sS$ and $\theta_1, \dots, \theta_k \geq 0$ with $\theta_1 + \dots + \theta_k = 1$. Then, \begin{align}
    f(\theta_1\vx_1 + \dots + \theta_k\vx_k) \leq \theta_1 f(\vx_1) + \dots + \theta_k f(\vx_k).
\end{align}
\end{thm}
\begin{proof}
We prove the statement by induction on $k$. The base case, $k = 2$, follows trivially from the convexity of $f$. For the induction step, suppose that the statement holds for some $k \geq 2$. Assume w.l.o.g. that $\theta_{k+1} \in (0,1)$. We have, \begin{align*}
    \sum_{i=1}^{k+1} \theta_i f(\vx_i) &= (1-\theta_{k+1})\parentheses*{\sum_{i=1}^k \frac{\theta_i}{1 - \theta_{k+1}} f(\vx_i)} + \theta_{k+1} f(\vx_{k+1}) \\
    &\geq (1-\theta_{k+1})f\parentheses*{\sum_{i=1}^k \frac{\theta_i}{1-\theta_{k+1}}\vx_i} + \theta_{k+1}f(\vx_{k+1}) \margintag{using the induction hypothesis} \\
    &\geq f\parentheses*{\sum_{i=1}^{k+1} \theta_i \vx_i}. \margintag{using convexity of $f$} &\qedhere
\end{align*}
\end{proof}