% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Linear Algebra}

If a square matrix $\mA \in \R^{n \times n}$ is symmetric\footnote{$\mA$ is symmetric iff $\mA = \trans{\mA}$.}, then $\mA$ has $n$ real \emph{eigenvalues} $\lambda_1, \dots, \lambda_n$ and \emph{eigenvectors} $\vv_1, \dots, \vv_n \in \R^n$ such that $\mA\vv_i = \lambda_i\vv_i$ and the $\vv_i$ are orthogonal\footnote{that is, $\trans{\vv_i}\vv_j = 0$ for $i \neq j$}.\footnote{We prove this in \cref{thm:a5}.}

\begin{defn}[Positive (semi-)definiteness]\index{positive definite matrix}\index{positive semi-definite matrix}\index{indefinite matrix}
Let $\mA \in \R^{n \times n}$ be a symmetric matrix. We say $\mA$ is \begin{enumerate}
    \item \emph{positive definite} iff $\trans{\vx}\mA\vx > 0$ for any $\vx \in \R^n \setminus \{\vZero\}$;
    \item \emph{positive semi-definite} iff $\trans{\vx}\mA\vx \geq 0$ for any $\vx \in \R^n$;
    \item if neither $\mA$ nor $-\mA$ is positive semi-definite, $\mA$ is \emph{indefinite}.
\end{enumerate}

We denote by $\sS^n$ the set of symmetric $n \times n$ matrices, by $\sS_+^n$ the set of such matrices that are positive semi-definite, and by $\sS_{++}^n$ the set of such matrices that are positive definite.
\end{defn}
\begin{thm}\label{thm:psd_eigenvalues}
Let $\mA \in \R^{n \times n}$ be a symmetric matrix. Then, \begin{enumerate}
    \item $\mA$ is positive definite iff all eigenvalues are positive; and
    \item $\mA$ is positive semi-definite iff all eigenvalues are non-negative.
\end{enumerate}
\end{thm}\noindent This theorem is a corollary of the Courant-Fischer theorem, which we will work towards now.

\begin{thm}[Spectral theorem for symmetric matrices]\index{spectral theorem for symmetric matrices} For all symmetric matrices $\mA \in \R^{n \times n}$ there exist \begin{align}
    \mV = \begin{bmatrix}
    \vv_1 & \cdots & \vv_n
    \end{bmatrix} \in \R^{n \times n}, \quad \text{and} \quad \mLambda = \diag\{\lambda_i\}_{i \in [n]} \in \R^{n \times n},
\end{align} where $\lambda_i$ and $\vv_i$ are the eigenvalues and corresponding (normalized) eigenvectors of $\mA$, such that \begin{enumerate}
    \item $\mA = \mV\mLambda\trans{\mV} = \sum_{i=1}^n \lambda_i \vv_i \trans{\vv_i}$; and
    \item $\trans{\mV}\mV = \mI$, i.e., the columns of $\mV$ form an orthonormal basis of $\R^n$.
\end{enumerate}
\end{thm}

\begin{thm}[Courant-Fischer min-max theorem]\index{Courant-Fischer min-max theorem} For symmetric matrices $\mA \in \R^{n \times n}$ with eigenvalues $\lambda_1 \leq \dots \leq \lambda_n$, \begin{align}
    \lambda_i &= \min_{\substack{\text{subspace $\sW \subseteq \R^n$} \\ \dim(\sW) = i}} \max_{\substack{\vx \in \sW \\ \vx \neq \vZero}} \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx} \label{eq:courant_fischer_1} \\
    &= \max_{\substack{\text{subspace $\sW \subseteq \R^n$} \\ \dim(\sW) = n - i + 1}} \min_{\substack{\vx \in \sW \\ \vx \neq \vZero}} \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx}.
\end{align}
\end{thm}
\begin{proof}
We show \cref{eq:courant_fischer_1}. The proof of the other equation proceeds analogously. \begin{itemize}
    \item ``$\geq$'': We choose $\sW = \vspan\{\vv_1, \dots, \vv_i\}$ We can write $\vx$ in the basis of eigenvectors, \begin{align*}
        \vx = \sum_{j=1}^i \vc(j) \vv_j
    \end{align*} for some $\vc \in \R^i$. We have, \begin{align*}
        \trans{\vx}\vx = \norm{\vx}_2^2 = \sum_{j=1}^i \sum_{k=1}^i \vc(j) \vc(k) \trans{\vv_j}\vv_k = \sum_{j=1}^i \vc(j)^2, \margintag{using that $\trans{\vv_j}\vv_k = 0$ if $j \neq k$ and $\trans{\vv_j}\vv_k = 1$ otherwise}
    \end{align*} and, \begin{align*}
        \trans{\vx}\mA\vx = \trans{\vx}\mV\mLambda\trans{\mV}\vx &= \trans{(\trans{\mV}\vx)}\mLambda(\underbrace{\trans{\mV}\vx}_{\vc}) \\ &= \trans{\vc}\mLambda\vc = \sum_{j=1}^i \lambda_j \vc(j)^2 \leq \lambda_i \sum_{j=1}^i \vc(j)^2.
    \end{align*} Altogether, \begin{align*}
        \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx} \leq \lambda_i.
    \end{align*}
    
    \item ``$\leq$'': Consider any subspace $\sW \subseteq \R^n$ with $\dim(\sW) = i$ and fix the subspace $\sT \defeq \vspan\{\vv_i, \dots, \vv_n\}$ with $\dim(\sT) = n - i + 1$. We have that $\dim(\sW \cap \sT) = \dim(\sW) + \dim(\sT) - \dim(\sW \cup \sT)$ and $\dim(\sW \cup \sT) \leq \dim(\R^n) = n$, so, $\dim(\sW \cap \sT) \geq 1$. Therefore, \begin{align*}
        \max_{\substack{\vx \in \sW \\ \vx \neq \vZero}} \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx} &\geq \max_{\substack{\vx \in \sW \cap \sT \\ \vx \neq \vZero}} \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx} \\
        &\geq \min_{\substack{\text{subspace $\sV \subseteq \sT$} \\ \dim(\sV) = 1}} \max_{\substack{\vx \in \sV \\ \vx \neq \vZero}} \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx}.
    \end{align*} For the last inequality note that $\sV$ can be chosen as $\sW \cap \sT$.
    
     We choose $\sV \defeq \vspan\{\vv_i\}$. For some $c \in \R$, we can write $\vx = c\vv_i$. Similarly to the previous part, we obtain, \begin{align*}
         \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx} = \frac{\lambda_i c^2}{c^2} = \lambda_i. &\qedhere
     \end{align*}
\end{itemize}
\end{proof}

\begin{proof}[Proof of \cref{thm:psd_eigenvalues}] Using Courant-Fischer, we have for the smallest eigenvalue $\lambda_1$ of $\mA$, \begin{align*}
    \lambda_1 = \min_{\substack{\vx \in \R^n \\ \vx \neq \vZero}} \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx}.
\end{align*} Thus, if $\lambda_1$ is positive, then $\trans{\vx}\mA\vx > 0$ for all $\vx \in \R^n \setminus \{\vZero\}$. In contrast, if for any such $\vx$, $\trans{\vx}\mA\vx > 0$, then $\lambda_1$ must be positive. The proof of positive semi-definiteness is analogous.
\end{proof}

\begin{cor}\label{lem:quadratic_form_lower_bound}
    For a symmetric matrix $\mA \in \R^{n \times n}$ and any $\vx \in \R^n$, \begin{align}
        \lambda_{\min}(\mA) \norm{\vx}_2^2 \leq \trans{\vx}\mA\vx,
    \end{align} where $\lambda_{\min}(\mA)$ is the smallest eigenvalue of $\mA$.
\end{cor}
\begin{proof}
By Courant-Fischer, we have for any $\vx \in \R^n$ such that $\vx \neq \vZero$, \begin{align*}
    \lambda_{\min}(\mA) = \min_{\substack{\vy \in \R^n \\ \vy \neq \vZero}} \frac{\trans{\vy}\mA\vy}{\norm{\vy}_2^2} \leq \frac{\trans{\vx}\mA\vx}{\norm{\vx}_2^2}.
\end{align*} If $\vx = \vZero$, the inequality trivially holds.
\end{proof}

\section{Matrix Norms}

\begin{defn}[Matrix norm]\index{matrix norm} Given a matrix $\mA \in \R^{n \times n}$ and norms $\norm{\cdot}_\alpha$ and $\norm{\cdot}_\beta$ on $\R^n$, the \emph{(induced) norm} of $\mA$ is defined as,\footnote{If you think of $\mA$ as a linear map, you can think of $\norm{\cdot}_\alpha$ as a norm of the input space and $\norm{\cdot}_\beta$ as a norm of the output space.} \begin{align}
    \norm{\mA}_{\alpha\to\beta} \defeq \sup_{\substack{\vx \in \R^n \\ \vx \neq \vZero}} \frac{\norm{\mA\vx}_\beta}{\norm{\vx}_\alpha}.
\end{align} We write $\norm{\mA}_\alpha \defeq \norm{\mA}_{\alpha\to\alpha}$.
\end{defn}

\begin{lem} For any matrix $\mA \in \R^{n \times n}$, any $\vx\ in \R^n$, and any norm $\norm{\cdot}$ on $\R^n$, \begin{align}
    |\trans{\vx}\mA\vx| \leq \norm{\mA}\norm{\vx}^2.
\end{align}
\end{lem}
\begin{proof} We have, \begin{align*}
    |\trans{\vx}\mA\vx| &\leq \norm{\vx} \norm{\mA\vx} \margintag{using Cauchy-Schwarz} \\
    &\leq \norm{\mA} \norm{\vx}^2. \margintag{using the definition of the induced matrix norm} \qedhere
\end{align*}
\end{proof}

\begin{lem} For a symmetric matrix $\mA \in \R^{n \times n}$, \begin{align}
    \norm{\mA}_2 = \max\{|\lambda_{\min}(\mA)|, |\lambda_{\max}(\mA)|\}.
\end{align} $\norm{\mA}_2$ is called the \emph{spectral norm}\index{spectral matrix norm} of $\mA$.
\end{lem}
\begin{proof}
We have, \begin{align*}
    \norm{\mA}_2^2 &= \sup_{\substack{\vx \in \R^n \\ \vx \neq \vZero}} \frac{\trans{\vx}\trans{\mA}\mA\vx}{\trans{\vx}\vx} \\
    &= \sup_{\substack{\vx \in \R^n \\ \vx \neq \vZero}} \frac{\trans{\vx}\mA^2\vx}{\trans{\vx}\vx} \margintag{using that $\mA$ is symmetric, $\trans{\mA} = \mA$} \\
    &= \sup_{\substack{\vx \in \R^n \\ \vx \neq \vZero}} \frac{\trans{\vx}\mV\mLambda^2\trans{\mV}\vx}{\trans{\vx}\vx} \margintag{using that $\mV$ is orthogonal, $\trans{\mV} = \inv{\mV}$} \\
    &= \sup_{\substack{\vx \in \R^n \\ \vx \neq \vZero}} \frac{\trans{\trans{\mV}\vx}\mLambda^2(\trans{\mV}\vx)}{\trans{\trans{\mV}\vx}(\trans{\mV}\vx)} \\
    &= \sup_{\substack{\vy \in \R^n \\ \vy \neq \vZero}} \frac{\trans{\vy}\mLambda^2\vy}{\trans{\vy}\vy} = \norm{\mLambda}_2^2. \margintag{using that the columns of $\mV$ form a basis of $\R^n$, set $\vy \defeq \trans{\mV}\vx$}
\end{align*} Finally, \begin{align*}
    \norm{\mLambda}_2^2 = \sup_{\substack{\vx \in \R^n \\ \vx \neq \vZero}} \frac{\trans{\vx}\mLambda^2\vx}{\trans{\vx}\vx} = \sup_{\substack{\vx \in \R^n \\ \vx \neq \vZero}} \frac{\sum_{i=1}^n \lambda_i^2 \vx(i)^2}{\sum_{i=1}^n \vx(i)^2} = \max_{i \in [n]} \lambda_i^2. &\qedhere
\end{align*}
\end{proof}

\section{Loewner Order}

The Loewner order (or positive semi-definite order) is a partial ordering on symmetric matrices.

\begin{defn}[Loewner order]\index{Loewner order}
Given symmetric matrices $\mA, \mB \in \R^{n \times n}$, $\mA \preceq \mB$ iff $\forall \vx \in \R^n: \trans{\vx}\mA\vx \leq \trans{\vx}\mB\vx$.
\end{defn}
\begin{rmk}
$\mA \succeq \mZero$ iff $\mA$ is positive semi-definite.
\end{rmk}

\begin{lem}[Properties of the Loewner order] We have that for any symmetric $\mA, \mB, \mC \in \R^{n \times n}$,
\begin{enumerate}
    \item $\mA \preceq \mA$ (reflexivity);
    \item $\mA \preceq \mB, \mB \preceq \mA \implies \mA = \mB$ (antisymmetry);
    \item $\mA \preceq \mB, \mB \preceq \mC \implies \mA \preceq \mC$ (transitivity);
    \item $\mA \preceq \mB \implies \mA + \mC \preceq \mB + \mC$;
    \item if $\mA \succeq \mZero$, then $\frac{1}{\alpha}\mA \preceq \mA \preceq \alpha\mA$ for any $\alpha \geq 1$; and
    \item if $\mA \preceq \mB$, then $\forall i \in [n]: \lambda_i(\mA) \leq \lambda_i(\mB)$, where $\lambda_i(\mM)$ is the $i$-th largest eigenvalue of $\mM$.\footnote{The converse is false: \begin{align*}
        \mA \defeq \begin{bmatrix}
            2 & 0 \\
            0 & 1 \\
        \end{bmatrix}, \quad \mB \defeq \begin{bmatrix}
            1 & 0 \\
            0 & 2 \\
        \end{bmatrix}
    \end{align*} have equal eigenvalues, but $\mA \not\preceq \mB$ and $\mB \not\preceq \mA$.}
\end{enumerate}
\end{lem}
\begin{rmk}
Properties (1), (2), and (3) together imply that the Loewner order is a partial ordering of symmetric matrices.
\end{rmk}
\begin{proof} Properties (1) through (5) follow directly from the definition using only elementary operations. For property (6), we have by Courant-Fischer, \begin{align*}
    \lambda_i(\mA) &= \min_{\substack{\text{subspace $\sW \subseteq \R^n$} \\ \dim(\sW) = i}} \max_{\substack{\vx \in \sW \\ \vx \neq \vZero}} \frac{\trans{\vx}\mA\vx}{\trans{\vx}\vx} \\
    &\leq \min_{\substack{\text{subspace $\sW \subseteq \R^n$} \\ \dim(\sW) = i}} \max_{\substack{\vx \in \sW \\ \vx \neq \vZero}} \frac{\trans{\vx}\mB\vx}{\trans{\vx}\vx} = \lambda_i(\mB). &\qedhere
\end{align*}
\end{proof}

\subsection{Loewner Order on Graphs}

\begin{defn} We write $G \preceq H$ iff we have for the Laplacian matrices $\mL_G$ and $\mL_H$ that $\mL_G \preceq \mL_H$. For any $c > 0$, we write $c G$ in place of $c \mL_G$, which corresponds to scaling the weight of every edge of $G$ by $c$.
\end{defn}
\begin{lem} For subgraphs $H \subseteq G$, we have $H \preceq G$.
\end{lem}
\begin{proof} Dropping edges can only decrease the quadratic form of the Laplacian of $G$ (see \cref{eq:laplacian_quadratic_form}).
\end{proof}

\section{Kernel and Image}

\begin{defn}[Kernel] The \emph{kernel}\index{kernel} (or \emph{null space}\index{null space}) of a linear map $\mA \in \R^{n \times m}$ is the linear subspace, \begin{align}
    \ker\mA \defeq \{\vx \in \R^m \mid \mA\vx = \vZero\}\subseteq \R^m.
\end{align}
\end{defn}
\begin{defn}[Image] The \emph{image}\index{image} (or \emph{range}\index{range}) of a linear map $\mA \in \R^{n \times m}$ is the linear subspace, \begin{align}
    \im\mA \defeq \{\mA\vx \mid \vx \in \R^m\} = \vspan\{\mA(:,1), \dots, \mA(:,m)\} \subseteq \R^n,
\end{align} where $\mA(:,i)$ denotes the $i$-th column vector of $\mA$.
\end{defn}

We have the following useful property relating image and kernel.

\begin{lem}
For a matrix $\mA \in \R^{n \times m}$, we have $\compl{\im(\trans{\mA})} = \ker\mA$ and $\compl{(\im\mA)} = \ker(\trans{\mA})$.\footnote{Note that $\compl{(\compl{\sW})} = \sW$ for any subspace $\sW$.}
\end{lem}
\begin{proof}
If $\vx \in \compl{\im(\trans{\mA})}$, we have $\trans{\mA(i,:)}\vx = 0$ for all $i \in [n]$. Hence, $\mA\vx = 0$ and $\vx \in \ker\mA$.

Conversely, if $\vx \in \ker\mA$, then $\mA\vx = \vZero$. Therefore, $\trans{\mA(i,:)}\vx = 0$ for all $i \in [n]$. As $\im(\trans{\mA}) = \vspan\{\mA(1,:), \dots, \mA(n,:)\}$, we have $\trans{\vz}\vx = 0$ for any $\vz \in \im(\trans{\mA})$, yielding, $\vx \in \compl{\im(\trans{\mA})}$.

The same argument goes through for the transpose of $\mA$ on both sides.
\end{proof}

\section{Matrix Functions}

First, let us remind ourselves of the notion of the trace of a matrix.

\begin{defn}[Trace] The \emph{trace}\index{trace} of a symmetric matrix $\mA \in \R^{n \times n}$ is defined as, \begin{align}
    \tr{\mA} \defeq \sum_{i = 1}^n \mA(i,i).
\end{align}
\end{defn}
\begin{lem} The trace is invariant under cyclic permutations. That is, for any $\mA, \mB \in \R^{n \times n}$, \begin{align}
    \tr{\mA\mB} = \tr{\mB\mA}.
\end{align}
\end{lem}
\begin{proof} We have, \begin{align*}
    \tr{\mA\mB} &= \sum_{i=1}^n (\mA\mB)(i,i) \\
    &= \sum_{i=1}^n \sum_{j=1}^n \mA(i,j)\mB(j,i) \\
    &= \sum_{j=1}^n \sum_{i=1}^n \mB(j,i)\mA(i,j) \\
    &= \sum_{j=1}^n (\mB\mA)(j,j) \\
    &= \tr{\mB\mA}. \qedhere
\end{align*}
\end{proof}
\begin{lem} For $\mA \in \R^{n \times n}$, \begin{align}
    \tr{\mA} = \sum_{i=1}^n \lambda_i,
\end{align} where $\lambda_i$ are the eigenvalues of $\mA$.
\end{lem}
\begin{proof} By the spectral theorem for symmetric matrices and using the cycle property of the trace, \begin{align*}
    \tr{\mA} = \tr{(\mV\mLambda\trans{\mV})} = \tr{(\mLambda\underbrace{\trans{\mV}\mV}_{\mI})} = \tr{\mLambda} = \sum_{i=1}^n \lambda_i,
\end{align*} where $\mLambda$ is a diagonal matrix of eigenvalues of $\mA$ and $\mV$ is an orthonormal matrix of the corresponding eigenvectors.
\end{proof}

\begin{defn}[Matrix function] A \emph{matrix function}\index{matrix function} $f : \sS^n \to \sS^n$ given the scalar function $f : \R \to \R$ is defined as, \begin{align}
    f(\mA) \defeq \mV \diag_i\{f(\lambda_i)\} \trans{\mV},
\end{align} where $\mA = \mV\diag_i\{\lambda_i\}\trans{\mV}$ is the spectral decomposition of $\mA$. We say, \begin{enumerate}
    \item a function $f: \spa{S} \to \R$ for $\spa{S} \subseteq \sS^n$ is \emph{monotonically increasing} iff $\mA \preceq \mB$ implies $f(\mA) \leq f(\mB)$; and similarly
    \item a matrix function $f: \spa{S} \to \spa{T}$ for $\spa{S}, \spa{T} \subseteq \sS^n$ is \emph{monotonically increasing} iff $\mA \preceq \mB$ implies $f(\mA) \preceq f(\mB)$.
\end{enumerate}
\end{defn}
\begin{lem}
If $f : \sT \to \R$ for $\sT \subseteq \R$ is monotone, then $\mX \mapsto \tr{f(\mX)}$ is monotone.
\end{lem}
\begin{proof}
Let $\mA \preceq \mB$. Then, $\lambda_i(\mA) \leq \lambda_i(\mB)$ for all $i \in [n]$. Thus, $f(\lambda_i(\mA)) \leq f(\lambda_i(\mB))$, and hence, \begin{align*}
    \tr{f(\mA)} = \sum_{i=1}^n f(\lambda_i(\mA)) \leq \sum_{i=1}^n f(\lambda_i(\mB)) = \tr{f(\mB)}.
\end{align*} The analogous argument works when $f$ is monotonically decreasing.
\end{proof}

\begin{lem}[Facts about matrix functions]
\leavevmode\begin{enumerate}
    \item $\mX \mapsto \inv{\mX}$ is monotonically decreasing on $\sS_{++}^n$;
    \item $\log$ is monotonically increasing on $\sS_{++}^n$;
    \item $\exp(\mA) \preceq \mI + \mA + \mA^2$ for $\norm{\mA}_2 \leq 1$;
    \item $\log(\mI + \mA) \preceq \mA$ for $\mA \succeq - \mI$; and
    \item (\emph{Lieb's theorem}\index{Lieb's theorem}) if $f(\mA) = \tr(\exp(\mH + \log(\mA)))$ for $\mA \in \sS_{++}^n$ and some $\mH \in \sS^n$, then $-f$ is convex.
\end{enumerate}
\end{lem}

\section{Pseudoinverses}

We often want to solve systems of linear equations such as \begin{align}
    \mA\vx = \vb \label{eq:linear_system}
\end{align} where $\mA$ and $\vb$ are given and we seek to identify $\vx$. If $\mA$ is invertible, a solution to \cref{eq:linear_system} is given by $\vx \defeq \inv{\mA}\vb$. Recall that a square matrix $\mA$ is invertible iff $\det\mA \neq 0$. Can we still explicitly write $\vx$ when $\mA$ is not invertible?

When $\mA$ is not invertible, the corresponding system of linear equations does not have a unique solution. However, we may still be able to find \emph{some} solution. The Moore-Penrose inverse is such a generalization of the inverse.

\begin{defn}[Moore-Penrose inverse] Given a symmetric matrix $\mA \in \R^{n \times n}$,\footnote{The Moore-Penrose inverse can also be defined for non-symmetric and even non-square matrices, but this will not be important for us.} its \emph{Moore-Penrose inverse}\index{Moore-Penrose inverse} (or simply \emph{pseudoinverse}\index{pseudoinverse}) is a matrix $\pinv{\mA} \in \R^{n \times n}$ such that \begin{enumerate}
    \item $\pinv{\mA}$ is symmetric;
    \item $\ker{\pinv{\mA}} = \ker{\mA}$;\footnote{that is, for any $\vv \in \R^n$, \begin{align*}
        \pinv{\mA}\vv = \vZero \iff \mA\vv = \vZero
    \end{align*}} and
    \item for $\vv \perp \ker{\mA}$, $\pinv{\mA}\mA\vv = \vv$ and $\mA\pinv{\mA}\vv = \vv$.
\end{enumerate}
\end{defn}

Thus, provided $\vb \perp \ker\mA$, $\vx \defeq \pinv{\mA}\vb$ is a solution to \cref{eq:linear_system}.

\begin{lem}
The pseudoinverse of the symmetric matrix $\mA \in \R^{n \times n}$ is (uniquely\footnote{this we will not prove}) characterized as, \begin{align}
    \pinv{\mA} = \mV\pinv{\mLambda}\trans{\mV},
\end{align} where $\mV$ is the matrix of orthogonal eigenvectors and \begin{align}
    \pinv{\mLambda} = \diag_{i\in[n]}\begin{cases}
    \inv{\lambda_i} & \lambda_i \neq 0 \\
    0 & \text{otherwise}
    \end{cases}
\end{align} for eigenvalues $\lambda_i$.
\end{lem}
\begin{proof}
The properties of a pseudoinverse follow from the definition immediately.
\end{proof}

\section{Solving Linear Systems}

Linear systems such as \cref{eq:linear_system} can often be solved much faster than by computing the pseudoinverse of $\mA$.

We first need to remind ourselves of the notion of a projection.

\begin{defn}[Projection matrix] A \emph{projection matrix}\index{projection matrix} is a matrix $\mPi \in \R^{n \times n}$ such that $\mPi^2 = \mPi$.\footnote{This property is called \emph{idempotency}\index{idempotency}.} We say that $\mPi$ is \emph{orthogonal} iff $\im\mPi \perp \ker\mPi$.
\end{defn}
\begin{rmk}
Note that $\mPi$ is the identity operator on $\im\mPi$, i.e., for all $\vx \in \im\mPi$ we have that $\mPi\vx = \vx$.
\end{rmk}

\begin{lem} Consider a real symmetric matrix $\mA = \mX\mY\trans{\mX}$, where $\mX$ is real and invertible and $\mY$ is real and symmetric. Let $\mPi_\mA$ denote the orthogonal projection to the image of $\mA$. Then, $\pinv{\mA} = \mPi_\mA\inv{(\trans{\mX})}\pinv{\mY}\inv{\mX}\mPi_\mA$.\footnote{Proof in \cref{lem:a6}.}
\end{lem}

Now, let us return to solving linear systems.

\begin{lem}
Given an invertible square lower triangular matrix $\mCalL$ or an invertible square upper triangular matrix $\mCalU$, we can solve the linear systems $\mCalL\vy = \vb$ and $\mCalU\vz = \vb$ in time $\LandauO{\nnz{\mCalL}}$ and $\LandauO{\nnz{\mCalU}}$, respectively, where $\nnz{\mA}$ denotes the number of non-zero entries of $\mA$.\footnote{We need the matrices to be stored as an adjacency list to have fast access to their non-zero entries.}
\end{lem}\begin{proof}[Proof sketch] We can iteratively solve the linear equations of $\mCalL\vy = \vb$. As $\mCalL$ has full rank and is lower triangular, there is always one linear equation in a single variable.

These algorithms are known as forward and back substitution for lower and upper triangular matrices, respectively.
\end{proof}

Thus, if we can decompose $\mA = \mCalL\trans{\mCalL}$ such that $\mCalL$ is invertible and lower triangular, we can solve the linear system of \cref{eq:linear_system} in time $\LandauO{\nnz{\mCalL}}$ by solving the two linear systems, \begin{align}
    \mCalL\vy &= \vb \quad\text{and} \\
    \trans{\mCalL}\vx &= \vy.
\end{align}

\begin{lem}[Cholesky decomposition]\index{Cholesky decomposition} For any positive semi-definite matrix $\mA \in \R^{n \times n}$, there is a decomposition of the form $\mA = \mCalL\trans{\mCalL}$ where $\mCalL \in \R^{n \times n}$ is lower triangular and positive semi-definite.
\end{lem}

We will see that the Cholesky decomposition can be computed efficiently when $\mA$ is a graph Laplacian.